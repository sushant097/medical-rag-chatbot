
# ğŸ©º Medical RAG Chatbot â€“ LangChain, Pinecone, Flask, AWS

A **Retrieval-Augmented Generation (RAG) Medical Chatbot** that answers user queries using a custom medical knowledge base.  
It combines **Large Language Models (LLMs)** with **vector search** to provide grounded, context-aware answers instead of relying purely on the modelâ€™s memory.

This project demonstrates **end-to-end AI application development**, covering data ingestion, embeddings, retrieval, UI, and cloud deployment.

---

## ğŸš€ Features

- **Retrieval-Augmented Generation (RAG):** Answers are grounded in a curated medical corpus.  
- **Custom Knowledge Base:** Documents embedded and stored in Pinecone for fast semantic search.  
- **Modern Web UI:** Flask + Bootstrap chat interface with dark/light mode and medical-themed styling.  
- **Cloud Native:** Containerized with Docker, deployable on AWS (ECR + EC2) via GitHub Actions CI/CD.  
- **Secure Setup:** Environment variables for API keys; `.gitignore` prevents sensitive files from leaking.  

---

## ğŸ§  How It Works (Approach)

1. **Load Data** â†’ Preprocess PDFs, FAQs, or notes into clean text chunks.  
2. **Generate Embeddings** â†’ Each chunk embedded with OpenAI embeddings.  
3. **Store in Pinecone** â†’ Embeddings + metadata are stored for similarity search.  
4. **Query Processing** â†’ User query hits Flask API, LangChain retrieves top-k matches from Pinecone.  
5. **RAG Pipeline** â†’ Retrieved context + query passed into GPT.  
6. **Answer Generation** â†’ GPT produces a grounded medical response.  
7. **Web UI** â†’ Flask renders answers in a chat interface with timestamps and disclaimers.  

---

## ğŸ“Š System Design

### ğŸ”„ RAG Runtime Flow
![](images/rag_runtime_block.png)  
When a user asks a question, Flask forwards it to LangChain, Pinecone retrieves relevant chunks, GPT generates an answer, and the response is sent back to the UI.

### ğŸ“¥ Data Ingestion Flow
![](images/data_ingestion_block.png)  
Medical documents are loaded, cleaned, chunked, embedded using OpenAI, and stored in Pinecone for retrieval.

### ğŸš€ CI/CD Deployment Flow
![](images/cicd_block.png)  
GitHub Actions builds and pushes Docker images to Amazon ECR.  
EC2 self-hosted runners pull and run the container, exposing the chatbot on port `8080`.

---

## ğŸ–¥ï¸ Demo

![](images/medichatbot.gif)

---

## ğŸ› ï¸ Tech Stack

- **Python 3.11**  
- **LangChain** â€“ RAG orchestration  
- **Pinecone** â€“ Vector database  
- **OpenAI GPT** â€“ LLM for natural language answers  
- **Flask** â€“ Backend + chat UI  
- **Bootstrap 5 / FontAwesome** â€“ UI styling  
- **Docker** â€“ Containerization  
- **AWS (ECR + EC2 + GitHub Actions)** â€“ Deployment pipeline  

---

## âš¡ Run Options

There are three easy ways to run this project:

### âœ… Option 1: Easiest (run.sh)
1. Create a `.env` file in the root:
   ```ini
   PINECONE_API_KEY="xxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
   OPENAI_API_KEY="xxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
    ````

2. Make the script executable and run:

   ```bash
   chmod +x run.sh
   ./run.sh
   ```

This will initialize Pinecone (if not already), install dependencies, and start the app.
Open [http://127.0.0.1:8080](http://127.0.0.1:8080).

---

### ğŸ“ Option 2: Manual Sequential Steps

For finer control, you can run everything step by step:

```bash
# 1. Clone
git clone https://github.com/sushant097/medical-rag-chatbot.git
cd medical-rag-chatbot

# 2. Create conda environment
conda create -n medichatbot python=3.11 -y
conda activate medichatbot

# 3. Install dependencies
pip install -r requirements.txt

# 4. Add .env file with Pinecone + OpenAI keys

# 5. Store embeddings
python store_index.py

# 6. Start app
python app.py
```

---

### ğŸ³ Option 3: Docker

If you prefer Dockerized deployment:

```bash
# Build image
docker build -t medibot .

# Run container with env vars from .env
docker run -d -p 8080:8080 --env-file .env medibot
```

---

## â˜ï¸ Deployment (AWS CI/CD)

This repo includes a **GitHub Actions workflow** (`.github/workflows/cicd.yaml`) for seamless deployment:

1. Build & push Docker image to **Amazon ECR**.
2. EC2 self-hosted runner pulls the image.
3. Flask app runs in a container on port `8080`.

Required GitHub Secrets:

* `AWS_ACCESS_KEY_ID`
* `AWS_SECRET_ACCESS_KEY`
* `AWS_DEFAULT_REGION`
* `ECR_REPO`
* `PINECONE_API_KEY`
* `OPENAI_API_KEY`

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ app.py               # Flask app (chat UI + backend)
â”œâ”€â”€ store_index.py       # Embed & upload docs to Pinecone
â”œâ”€â”€ src/                 # Utilities & prompt templates
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ prompt.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ templates/           # HTML templates (chat UI)
â”‚   â””â”€â”€ chat.html
â”œâ”€â”€ static/              # CSS/JS assets
â”‚   â””â”€â”€ chat.css
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ run.sh               # Bootstrap & run script
â”œâ”€â”€ Dockerfile           # Docker build
â”œâ”€â”€ .env.example         # Example env vars
â”œâ”€â”€ .gitignore           # Ignore secrets & caches
â””â”€â”€ README.md            # Project documentation
```

---

## ğŸ“œ License

[MIT](LICENSE)

